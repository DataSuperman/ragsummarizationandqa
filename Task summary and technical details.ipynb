{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915d4604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_handling import WikiHandler\n",
    "from src.config import AppResources\n",
    "from pathlib import Path\n",
    "from src.summarizer import OpenAISummarizer\n",
    "from src.question_answering_system import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b6422f",
   "metadata": {},
   "source": [
    "## Summarizer task\n",
    "\n",
    "To run this task we need three classes:\n",
    "<ol>\n",
    "<li><strong>DataHandler:</strong> This class wraps the concept of handling the input json and converting the various segments of conversation in unified text. It also internally combines consecutive conversations of each speaker which might be splitted.</li>\n",
    "<li><strong>AppResources:</strong> This class contains all the resources that will be needed by the summarizer. All the resources are mentioned into a config.json file. This class needs the config.json for instantiation </li>\n",
    "<li><strong>OpenAISummarizer:</strong> This class contains all the logic for summarization, including three techniques for summarization namely:\n",
    "    <ol>\n",
    "        <li><strong>StuffSummarizer</strong></li>\n",
    "        <li><strong>MapReduceSummarizer</strong></li>\n",
    "        <li><strong>RefineSummarizer</strong></li>\n",
    "    </ol>\n",
    "    </li>\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fea2f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/extravolatile/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /home/extravolatile/.local/lib/python3.10/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "# create a datahandler, it \n",
    "dataset = WikiHandler.get_data(\"Mughals\")\n",
    "# Create Resources object, this class abstracts away everything that the summarizer will need\n",
    "app_res = AppResources.from_config_file(config_path=Path('config_files/config.json'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ff3f773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7506a2bc",
   "metadata": {},
   "source": [
    "#### Stuff Summarizer\n",
    "Idea is simple if the model permits just using all the text into a single call for summarization. Unfortunately gpt3.5 has a maximum token length of 16k tokens for a single call and our demo text has closer to 25k tokens. So in the code if somebody provides text longer than 16k tokens and specifies <strong>stuff</strong>, it will fall back to <strong>mapreducesummarizer</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06f543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = OpenAISummarizer(app_res,summarizer_type='stuff')\n",
    "summary =  summarizer.summarize(dataset.data_as_str)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53636c47",
   "metadata": {},
   "source": [
    "#### MapReduce Summarizer\n",
    "\n",
    "Idea here is split text into multiple smaller chunks, call summarization into each of them. Then combine the intermediate summarizations to a single final summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7156ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = OpenAISummarizer(app_res,summarizer_type='mapreduce')\n",
    "summary =  summarizer.summarize(dataset.data_as_str)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7802f23e",
   "metadata": {},
   "source": [
    "#### Refine Summarizer\n",
    "Idea here is to split text into multiple smaller chunks, call summarization into the first chunk and then append the summarization of the previous chunks into the new one till we reach the final summary. This process is the most expensive of all the three summarizers, as we have a linear stack of calls.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbf9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = OpenAISummarizer(app_res,summarizer_type='refine')\n",
    "summary =  summarizer.summarize(dataset.data_as_str)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42376935",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Question Answering task\n",
    "\n",
    "To run this task we need three classes:\n",
    "<ol>\n",
    "<li><strong>DataHandler:</strong> This class wraps the concept of handling the input json and converting the various segments of conversation in unified text. It also internally combines consecutive conversations of each speaker which might be splitted.</li>\n",
    "<li><strong>AppResources:</strong> This class contains all the resources that will be needed by the QA system. All the resources are mentioned into a config.json file. This class needs the config.json for instantiation </li>\n",
    "<li><strong>QuestionAnsweringSystem:</strong> This class contains all the logic for QA system to process. This class also employs some verification frameworks. There is logic that can verify whether answers are hallucinated by the model or not, if the generated text has any relevance to the question or not. \n",
    "<li><strong>VectorDatabase:</strong> This class is used to transform our documents into embeddings, so we can  semantically find related answers to the questions. All of the data is not persisted and only stored in memory. Default database I used here is <strong>FAISS</strong>  </li> \n",
    "    \n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f485797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a datahandler, it \n",
    "dataset = DataHandler(\"datasets/demo-segments.json\")\n",
    "\n",
    "# Create Resources object, this class abstracts away everything that the summarizer will need\n",
    "app_res = AppResources.from_config_file(config_path=Path('config_files/config.json'))\n",
    "\n",
    "# create and store the embeddings of chunked text into the database.\n",
    "db = VectorDatabase(app_res)\n",
    "db.add_documents(dataset.data_as_str)\n",
    "\n",
    "# create the retriever \n",
    "retriever = db.get_retriever()\n",
    "\n",
    "# Instantiate the QAsystem\n",
    "qa_system = QuestionAnsweringSystem(app_res, retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea502f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the question\n",
    "question = \"Who is Lancelot? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804867c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answer = qa_system.run(question)\n",
    "print(f\"Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbc8ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
